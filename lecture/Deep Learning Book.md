# Deep Learning Book

## Norm Penalties as Constrained Optimization

如果我们想限制参数矩阵的大小，使其小于一个定值，可以考虑用拉格朗日乘子法，即把参数的norm和定值的差作为惩罚加在目标函数上，选择惩罚项的系数最大化目标函数，然后最小化选择惩罚项系数之后的目标函数。

实际应用中，这种限制其实可以用更粗暴的方法解决，那就是每次梯度迭代之后强行把参数投射到最近的满足限制条件的点上。这种方法往往更稳定，而且避免了拉格朗日乘子法造成的目标函数非凸的问题。

## Regularization and Under-constraint Problems

规范化可以用来解决问题的解限制过少的问题。

## Dataset Augmentation

当手里的数据太少时，你可以造点数据塞给它！

不同的问题自己产生数据难度不同。

比如图像识别，可以把手里的图像旋转平移等等，还可以加点噪声，这还能提高模型的抗噪声能力。

## Noise Robustness

在输入上加噪声是一种提高模型效率的策略。当噪声的量接近无穷小时，这等价于增加一项对范数的限制。不过，加噪声往往比直接缩小参数更加有效，特别是在中间层中加噪声。

另外还可以在被训练的参数上加噪声。这等价于贝叶斯推导的随机实现。同样，当噪声比较小时，这相当于加了一个规范项。这可以促使模型将参数优化到不仅是局部最小值，而且这个局部最小值附近目标变化的幅度也很小，即目标函数比较平缓。

## Multi-task Learning

同一个输入，多个输出。

有些参数是被多个任务共享的。

## Early Stopping

当模型的参数很多时，往往会over fit。这种情况下，随着训练时间增长，训练误差会稳定下降，但验证误差会先减后增。Early stopping的思想就是不要训练太久，干脆让参数停在验证误差开始增长之前。这也是一种规范化方法，简单有效。

Early stopping需要把一部分训练数据用做验证，因而没有把它们用来训练。为了更充分地利用这些数据，可以进行一次额外的训练。一种方法是得到early stopping的最优步数之后，再重新初始化参数，并用得到的最优步数和全部训练数据重新训练。另一种方法是在第一次训练得到的参数上继续训练，只是这次把第一次用作验证的数据也用来训练，这时候就没有步数限制了，而是等训练误差低于early stopping停止时的误差时就停止。

在简单的线性模型下，可以证明early stopping和$L^2$规范化是等价的。